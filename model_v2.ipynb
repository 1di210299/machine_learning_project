{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded - ELF shape: (453, 322), MAG shape: (938, 322)\n",
      "\n",
      "Adding statistical features...\n",
      "\n",
      "Adding statistical features...\n",
      "\n",
      "Performing grid search for elf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:26<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing grid search for mag...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:58<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation results for elf:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99        76\n",
      "           1       0.98      0.98      0.98        65\n",
      "\n",
      "    accuracy                           0.99       141\n",
      "   macro avg       0.99      0.99      0.99       141\n",
      "weighted avg       0.99      0.99      0.99       141\n",
      "\n",
      "\n",
      "Evaluation results for mag:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       137\n",
      "           1       1.00      0.99      0.99       167\n",
      "\n",
      "    accuracy                           0.99       304\n",
      "   macro avg       0.99      0.99      0.99       304\n",
      "weighted avg       0.99      0.99      0.99       304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import joblib\n",
    "import os\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "class ModelPipeline:\n",
    "    def __init__(self, target_size: int = 320):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.target_size = target_size\n",
    "        self.models = {\n",
    "            'elf': None,\n",
    "            'mag': None\n",
    "        }\n",
    "        \n",
    "    def load_data(self, data_dir: str = 'data') -> Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame]]:\n",
    "        \"\"\"Load and validate data from directory structure\"\"\"\n",
    "        try:\n",
    "            # Cargar todos los archivos CSV de ELF\n",
    "            elf_files = os.listdir(os.path.join(data_dir, 'ELF'))\n",
    "            elf_data = []\n",
    "            for file in elf_files:\n",
    "                if file.endswith('.csv'):\n",
    "                    df = pd.read_csv(os.path.join(data_dir, 'ELF', file))\n",
    "                    elf_data.append(df)\n",
    "            data_elf = pd.concat(elf_data, ignore_index=True) if elf_data else None\n",
    "\n",
    "            # Cargar todos los archivos CSV de MAG\n",
    "            mag_files = os.listdir(os.path.join(data_dir, 'MAG'))\n",
    "            mag_data = []\n",
    "            for file in mag_files:\n",
    "                if file.endswith('.csv'):\n",
    "                    df = pd.read_csv(os.path.join(data_dir, 'MAG', file))\n",
    "                    mag_data.append(df)\n",
    "            data_mag = pd.concat(mag_data, ignore_index=True) if mag_data else None\n",
    "\n",
    "            if data_elf is not None and data_mag is not None:\n",
    "                print(f\"Data loaded - ELF shape: {data_elf.shape}, MAG shape: {data_mag.shape}\")\n",
    "            return data_elf, data_mag\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def standardize_samples(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Standardize data to target size using padding or truncation\"\"\"\n",
    "        features = data.select_dtypes(include=[np.number]).columns\n",
    "        data_features = data[features]\n",
    "        \n",
    "        if data_features.shape[1] < self.target_size:\n",
    "            padding = pd.DataFrame(0, index=data_features.index, \n",
    "                                 columns=[f'padded_{i}' for i in range(data_features.shape[1], self.target_size)])\n",
    "            return pd.concat([data_features, padding], axis=1)\n",
    "        elif data_features.shape[1] > self.target_size:\n",
    "            return data_features.iloc[:, :self.target_size]\n",
    "        return data_features\n",
    "\n",
    "    def add_features(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Add statistical features to the dataset\"\"\"\n",
    "        print(\"\\nAdding statistical features...\")\n",
    "        features = pd.DataFrame()\n",
    "        features['mean'] = X.mean(axis=1)\n",
    "        features['std'] = X.std(axis=1)\n",
    "        features['max'] = X.max(axis=1)\n",
    "        features['min'] = X.min(axis=1)\n",
    "        features['median'] = X.median(axis=1)\n",
    "        features['skew'] = X.skew(axis=1)\n",
    "        features['kurtosis'] = X.kurtosis(axis=1)\n",
    "        return pd.concat([X, features], axis=1)\n",
    "\n",
    "    def preprocess_data(self, data: pd.DataFrame, is_training: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"Preprocess the data including normalization and feature engineering\"\"\"\n",
    "        # Separate features and labels\n",
    "        if 'Label' in data.columns:\n",
    "            X = data.drop('Label', axis=1)\n",
    "            y = data['Label']\n",
    "        else:\n",
    "            X = data\n",
    "            y = None\n",
    "\n",
    "        # Standardize sample size\n",
    "        X = self.standardize_samples(X)\n",
    "        \n",
    "        # Normalize\n",
    "        if is_training:\n",
    "            X = pd.DataFrame(self.scaler.fit_transform(X), columns=X.columns)\n",
    "        else:\n",
    "            X = pd.DataFrame(self.scaler.transform(X), columns=X.columns)\n",
    "\n",
    "        # Add features\n",
    "        X = self.add_features(X)\n",
    "\n",
    "        if y is not None:\n",
    "            return X, y\n",
    "        return X\n",
    "\n",
    "    def train_model(self, X: pd.DataFrame, y: pd.DataFrame, sensor_type: str):\n",
    "        \"\"\"Train the model with grid search and cross validation\"\"\"\n",
    "        # Create ensemble\n",
    "        clf1 = RandomForestClassifier(random_state=42)\n",
    "        clf2 = GradientBoostingClassifier(random_state=42)\n",
    "        clf3 = SVC(probability=True, random_state=42)\n",
    "\n",
    "        ensemble = VotingClassifier(\n",
    "            estimators=[('rf', clf1), ('gb', clf2), ('svc', clf3)],\n",
    "            voting='soft'\n",
    "        )\n",
    "\n",
    "        # Parameters for grid search\n",
    "        param_grid = {\n",
    "            'rf__n_estimators': [100, 200],\n",
    "            'rf__max_depth': [10, 20, None],\n",
    "            'rf__min_samples_split': [2, 5]\n",
    "        }\n",
    "\n",
    "        # Perform grid search\n",
    "        print(f\"\\nPerforming grid search for {sensor_type}...\")\n",
    "        grid_search = GridSearchCV(ensemble, param_grid, cv=5, n_jobs=-1)\n",
    "        with tqdm(total=100) as pbar:\n",
    "            grid_search.fit(X, y)\n",
    "            for i in range(100):\n",
    "                time.sleep(0.1)\n",
    "                pbar.update(1)\n",
    "\n",
    "        self.models[sensor_type] = grid_search\n",
    "        return grid_search\n",
    "\n",
    "    def evaluate_model(self, X_test: pd.DataFrame, y_test: pd.DataFrame, sensor_type: str):\n",
    "        \"\"\"Evaluate the model and display results\"\"\"\n",
    "        model = self.models[sensor_type]\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(f\"\\nEvaluation results for {sensor_type}:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        # Crear directorio para los plots\n",
    "        plots_dir = os.path.join('train_plots', sensor_type.upper())\n",
    "        os.makedirs(plots_dir, exist_ok=True)\n",
    "        \n",
    "        # Confusion matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d')\n",
    "        plt.title(f'Confusion Matrix - {sensor_type}')\n",
    "        plt.ylabel('True')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.savefig(os.path.join(plots_dir, f'confusion_matrix_{sensor_type}.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # Guardar métricas en archivo de texto\n",
    "        metrics_file = os.path.join(plots_dir, f'metrics_{sensor_type}.txt')\n",
    "        with open(metrics_file, 'w') as f:\n",
    "            f.write(f\"Evaluation results for {sensor_type}:\\n\")\n",
    "            f.write(\"\\nClassification Report:\\n\")\n",
    "            f.write(classification_report(y_test, y_pred))\n",
    "            f.write(f\"\\nAccuracy: {(y_pred == y_test).mean():.4f}\")\n",
    "\n",
    "    def save_models(self, base_dir: str = 'model'):\n",
    "        \"\"\"Save trained models and scaler in appropriate directories\"\"\"\n",
    "        # Guardar modelo ELF\n",
    "        if self.models['elf'] is not None:\n",
    "            elf_dir = os.path.join(base_dir, 'ELF')\n",
    "            os.makedirs(elf_dir, exist_ok=True)\n",
    "            joblib.dump(self.models['elf'], os.path.join(elf_dir, 'model_elf.joblib'))\n",
    "            joblib.dump(self.scaler, os.path.join(elf_dir, 'scaler_elf.joblib'))\n",
    "\n",
    "        # Guardar modelo MAG\n",
    "        if self.models['mag'] is not None:\n",
    "            mag_dir = os.path.join(base_dir, 'MAG')\n",
    "            os.makedirs(mag_dir, exist_ok=True)\n",
    "            joblib.dump(self.models['mag'], os.path.join(mag_dir, 'model_mag.joblib'))\n",
    "            joblib.dump(self.scaler, os.path.join(mag_dir, 'scaler_mag.joblib'))\n",
    "\n",
    "    def load_models(self, base_dir: str = 'model'):\n",
    "        \"\"\"Load trained models and scalers from appropriate directories\"\"\"\n",
    "        # Cargar modelo ELF\n",
    "        elf_model_path = os.path.join(base_dir, 'ELF', 'model_elf.joblib')\n",
    "        elf_scaler_path = os.path.join(base_dir, 'ELF', 'scaler_elf.joblib')\n",
    "        if os.path.exists(elf_model_path):\n",
    "            self.models['elf'] = joblib.load(elf_model_path)\n",
    "            if os.path.exists(elf_scaler_path):\n",
    "                self.scaler = joblib.load(elf_scaler_path)\n",
    "\n",
    "        # Cargar modelo MAG\n",
    "        mag_model_path = os.path.join(base_dir, 'MAG', 'model_mag.joblib')\n",
    "        mag_scaler_path = os.path.join(base_dir, 'MAG', 'scaler_mag.joblib')\n",
    "        if os.path.exists(mag_model_path):\n",
    "            self.models['mag'] = joblib.load(mag_model_path)\n",
    "            if os.path.exists(mag_scaler_path):\n",
    "                self.scaler = joblib.load(mag_scaler_path)\n",
    "\n",
    "def main():\n",
    "    # Initialize pipeline\n",
    "    pipeline = ModelPipeline()\n",
    "\n",
    "    # Load data from directory structure\n",
    "    data_elf, data_mag = pipeline.load_data('data')\n",
    "\n",
    "    if data_elf is None or data_mag is None:\n",
    "        print(\"Error loading data. Exiting...\")\n",
    "        return\n",
    "\n",
    "    # Process ELF data\n",
    "    X_elf, y_elf = pipeline.preprocess_data(data_elf)\n",
    "    \n",
    "    # Process MAG data\n",
    "    X_mag, y_mag = pipeline.preprocess_data(data_mag)\n",
    "\n",
    "    # Apply SMOTEENN\n",
    "    smote_enn = SMOTEENN(random_state=42)\n",
    "    X_elf_balanced, y_elf_balanced = smote_enn.fit_resample(X_elf, y_elf)\n",
    "    X_mag_balanced, y_mag_balanced = smote_enn.fit_resample(X_mag, y_mag)\n",
    "\n",
    "    # Split data\n",
    "    X_train_elf, X_test_elf, y_train_elf, y_test_elf = train_test_split(\n",
    "        X_elf_balanced, y_elf_balanced, test_size=0.2, random_state=42)\n",
    "    X_train_mag, X_test_mag, y_train_mag, y_test_mag = train_test_split(\n",
    "        X_mag_balanced, y_mag_balanced, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train models\n",
    "    pipeline.train_model(X_train_elf, y_train_elf, 'elf')\n",
    "    pipeline.train_model(X_train_mag, y_train_mag, 'mag')\n",
    "\n",
    "    # Evaluate models\n",
    "    pipeline.evaluate_model(X_test_elf, y_test_elf, 'elf')\n",
    "    pipeline.evaluate_model(X_test_mag, y_test_mag, 'mag')\n",
    "\n",
    "    # Save models in appropriate directories\n",
    "    pipeline.save_models('model')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
